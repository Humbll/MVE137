{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42d4e64",
   "metadata": {},
   "source": [
    "**Project Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325c54a",
   "metadata": {},
   "source": [
    "**Formalities**\n",
    "\n",
    "This is the project for the course Probability and Statistical Learning Using Python, 2023. Here, you are asked to carry out the analysis using the tools, techniques, and skills acquired in the course and hand in a .pynb file with the solutions.\n",
    "\n",
    "The **deadline  is Friday, October\n",
    "27, 2023.** You should upload the solution file to 'Project Assignment' in Canvas via 'Home-->Project Assignment'.\n",
    "Note that this is an individual exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab58442",
   "metadata": {},
   "source": [
    "**Part I**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a735f",
   "metadata": {},
   "source": [
    "In this exercise we will estimate the test error of logistic regression model using the below described validation set approach. You will neeed to import the *Default.csv* file provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "679f0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "95b68f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "#Uncomment and set the path to the csv file below\n",
    "df = pd.read_csv(\"Default.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6928055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    default student      balance       income\n",
      "1        No      No   729.526495  44361.62507\n",
      "2        No     Yes   817.180407  12106.13470\n",
      "3        No      No  1073.549164  31767.13895\n",
      "4        No      No   529.250605  35704.49394\n",
      "5        No      No   785.655883  38463.49588\n",
      "..      ...     ...          ...          ...\n",
      "96       No      No   820.017113  51584.65732\n",
      "97       No     Yes   619.751869  15750.62208\n",
      "98       No      No  1047.718124  46416.97099\n",
      "99       No      No   243.841328  47193.88813\n",
      "100      No      No   186.500387  45430.55027\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.head(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e4c6d",
   "metadata": {},
   "source": [
    "(a) Fit a logistic regression model that uses $income$ and $balance$ to\n",
    "predict $default$ and print out the summary. **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "786cea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "Model Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Fri, 27 Oct 2023   Pseudo R-squ.:                  0.4594\n",
      "Time:                        21:35:19   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "Accuracy: 0.966\n",
      "Test Error: 0.03400000000000003\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Default.csv')\n",
    "\n",
    "# Map the file\n",
    "df['default'] = df['default'].map({'No': 0, 'Yes': 1})\n",
    "df['student'] = df['student'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Define X and Y\n",
    "X = df[['income', 'balance']]\n",
    "y = df['default']\n",
    "\n",
    "# Create training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "# logistic regression model\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# Prediction step\n",
    "y_pred = logit.predict(X_test)\n",
    "\n",
    "\n",
    "model_summary = smf.logit('default ~ income + balance', data=df).fit().summary()\n",
    "print(\"Model Summary:\")\n",
    "print(model_summary)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "test_error = 1 - accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "print('Test Error:', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302291b",
   "metadata": {},
   "source": [
    "(b) You are supposed to estimate the test error of this model using the validation set approach described below. In order to do this, you must perform the following steps: **(4 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31129a5",
   "metadata": {},
   "source": [
    "i. Split the sample set into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fff91e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41deb6",
   "metadata": {},
   "source": [
    "ii. Fit a multiple logistic regression model using only the training\n",
    "observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "52306566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model = LogisticRegression()\n",
    "logit_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e13126",
   "metadata": {},
   "source": [
    "iii. Obtain a prediction of default status for each individual in\n",
    "the validation set (test set) by computing the posterior probability of\n",
    "$default$ for that individual, and classifying the individual to\n",
    "the $default$ category if the posterior probability is greater\n",
    "than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d2e4618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction= logit_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bfa3b",
   "metadata": {},
   "source": [
    "iv. Compute the validation set error, which is the fraction of\n",
    "the observations in the validation set (test set) that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a636670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Accuracy: 0.9671428571428572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_accuracy = accuracy_score(y_val, y_prediction,)\n",
    "print('Validation Set Accuracy:', validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f375ea",
   "metadata": {},
   "source": [
    "(c) Now consider a logistic regression model that predicts the probability of default using $income$, $balance$, a dummy variable for $student$ and print the summary. Estimate the test error for this model using the validation\n",
    "set approach. Comment on the results. Does the inclusion of a dummy variable for student lead to a reduction in the test error? **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d0e54900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9663333333333334\n",
      "Test Error:  0.03366666666666662\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078577\n",
      "         Iterations 10\n",
      "Summary:                             Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9996\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 27 Oct 2023   Pseudo R-squ.:                  0.4619\n",
      "Time:                        21:38:58   Log-Likelihood:                -785.77\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                3.257e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -10.8690      0.492    -22.079      0.000     -11.834      -9.904\n",
      "income      3.033e-06    8.2e-06      0.370      0.712    -1.3e-05    1.91e-05\n",
      "balance        0.0057      0.000     24.737      0.000       0.005       0.006\n",
      "student       -0.6468      0.236     -2.738      0.006      -1.110      -0.184\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.15 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "X = df[['income']]\n",
    "y = df['default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Test Error: \", 1 - accuracy_score(y_test, y_pred))\n",
    "logit_summary = smf.logit('default ~ income + balance + student', data=df).fit().summary()\n",
    "print(\"Summary: \", logit_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f6b09",
   "metadata": {},
   "source": [
    "**Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4eb2b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f0854",
   "metadata": {},
   "source": [
    "In this exercise, you will demonstrate your understanding of the KNN classification algorithm and test it on a breast cancer dataset. The algorithm should be implemented in pure python, without using the sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ce805",
   "metadata": {},
   "source": [
    "**KNN algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048b3fc",
   "metadata": {},
   "source": [
    "(a)  Implement a function of your own to perform KNN classification **without using the default available libraries such as KNeighborsClassifier() in sklearn**. You will need to consider the Euclidean distances between the features and data to be predicted (test data) when selecting the k-nearest neighbors. The function should take 3 inputs as 1) data set to train the model 2) data to test 3) number of neighbours (k). If *k* is set to a value less than or equal to the total classification groups, the function should give a warning, and  warn() function defined in the 'warning' module will be useful for that.\n",
    "\n",
    "The function should output *classification_result*, where *classification_result* is the result of your classifier. Further, you should provide a suitable measure of the confidence on the classification. Justify your choice. \n",
    "\n",
    "Please note that only a few basic libraries/modules are imported and thus you are expected to import the needed others. \n",
    "\n",
    "\n",
    "\n",
    "**Hint:** You may fill and complete the function given below.                **(5 pts)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d468242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_algorithm(train_data, test_data, k_neighbors=5):\n",
    "    num_classes = len(np.unique(train_data[:, -1]))\n",
    "\n",
    "    if k_neighbors >= len(train_data):\n",
    "        warnings.warn(\"k is greater than or equal to the number of data points in the training set. Consider choosing a smaller k.\")\n",
    "        k_neighbors = len(train_data) - 1\n",
    "\n",
    "    classification_result = np.zeros(len(test_data), dtype=int)\n",
    "    confidence = np.zeros(len(test_data), dtype=float)\n",
    "\n",
    "    for i, test_instance in enumerate(test_data):\n",
    "        distances = np.sqrt(np.sum((train_data[:, :-1] - test_instance) ** 2, axis=1))\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        k_indices = sorted_indices[:k_neighbors]\n",
    "        k_nearest_labels = train_data[k_indices, -1]\n",
    "\n",
    "        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "        most_common_label = unique_labels[np.argmax(counts)]\n",
    "\n",
    "        classification_result[i] = most_common_label\n",
    "        confidence[i] = counts.max() / k_neighbors\n",
    "\n",
    "    return classification_result, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b1202",
   "metadata": {},
   "source": [
    "#### Now let's test the implemented KNN algorithm on the given breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd0f63",
   "metadata": {},
   "source": [
    "This dataset contains records of breast cancer patients. Here we will use the features (columns) to predict the correct cancer class (last column) for the patients in the dataset as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "27af2716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhesion</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chrom</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>776715</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>841769</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>888820</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  clump_thickness  unif_cell_size  unif_cell_shape  marg_adhesion  \\\n",
       "0    1000025                5               1                1              1   \n",
       "1    1002945                5               4                4              5   \n",
       "2    1015425                3               1                1              1   \n",
       "3    1016277                6               8                8              1   \n",
       "4    1017023                4               1                1              3   \n",
       "..       ...              ...             ...              ...            ...   \n",
       "694   776715                3               1                1              1   \n",
       "695   841769                2               1                1              1   \n",
       "696   888820                5              10               10              3   \n",
       "697   897471                4               8                6              4   \n",
       "698   897471                4               8                8              5   \n",
       "\n",
       "     single_epith_cell_size bare_nuclei  bland_chrom  norm_nucleoli  mitoses  \\\n",
       "0                         2           1            3              1        1   \n",
       "1                         7          10            3              2        1   \n",
       "2                         2           2            3              1        1   \n",
       "3                         3           4            3              7        1   \n",
       "4                         2           1            3              1        1   \n",
       "..                      ...         ...          ...            ...      ...   \n",
       "694                       3           2            1              1        1   \n",
       "695                       2           1            1              1        1   \n",
       "696                       7           3            8             10        2   \n",
       "697                       3           4           10              6        1   \n",
       "698                       4           5           10              4        1   \n",
       "\n",
       "     class  \n",
       "0        2  \n",
       "1        2  \n",
       "2        2  \n",
       "3        2  \n",
       "4        2  \n",
       "..     ...  \n",
       "694      2  \n",
       "695      2  \n",
       "696      4  \n",
       "697      4  \n",
       "698      4  \n",
       "\n",
       "[699 rows x 11 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['id', 'clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion',\n",
    "           'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses', 'class']\n",
    "#Import the data file by uncommenting below and setting the path to the dataset\n",
    "df = pd.read_csv('breast-cancer-wisconsin.data', header=None, names=columns)\n",
    "df.head(699)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb999d6b",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be7ad9",
   "metadata": {},
   "source": [
    "(b) Check how many different cancer classes are available, and plot a pie chart to see the distribution of classes. Then, find and replace all the missing values with the mode of the particular column(s). Note that the missing values are marked with '?' in the dataset. **(1 pt)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e4883d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of classes 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = df['class'].value_counts()\n",
    "print(\"amount of classes\", num_classes)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Distribution Cancer Classes\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df = df.replace('?', pd.NA)\n",
    "df = df.fillna(df.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80206cd",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a7b6c",
   "metadata": {},
   "source": [
    "(c) Drop the obiviously unwanted column(s). **(1 pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67debe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhesion</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chrom</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     clump_thickness  unif_cell_size  unif_cell_shape  marg_adhesion  \\\n",
       "0                  5               1                1              1   \n",
       "1                  5               4                4              5   \n",
       "2                  3               1                1              1   \n",
       "3                  6               8                8              1   \n",
       "4                  4               1                1              3   \n",
       "..               ...             ...              ...            ...   \n",
       "694                3               1                1              1   \n",
       "695                2               1                1              1   \n",
       "696                5              10               10              3   \n",
       "697                4               8                6              4   \n",
       "698                4               8                8              5   \n",
       "\n",
       "     single_epith_cell_size bare_nuclei  bland_chrom  norm_nucleoli  mitoses  \\\n",
       "0                         2           1            3              1        1   \n",
       "1                         7          10            3              2        1   \n",
       "2                         2           2            3              1        1   \n",
       "3                         3           4            3              7        1   \n",
       "4                         2           1            3              1        1   \n",
       "..                      ...         ...          ...            ...      ...   \n",
       "694                       3           2            1              1        1   \n",
       "695                       2           1            1              1        1   \n",
       "696                       7           3            8             10        2   \n",
       "697                       3           4           10              6        1   \n",
       "698                       4           5           10              4        1   \n",
       "\n",
       "     class  \n",
       "0        2  \n",
       "1        2  \n",
       "2        2  \n",
       "3        2  \n",
       "4        2  \n",
       "..     ...  \n",
       "694      2  \n",
       "695      2  \n",
       "696      4  \n",
       "697      4  \n",
       "698      4  \n",
       "\n",
       "[699 rows x 10 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #use df drop to drop column id since its unwanted \n",
    "df = df.drop(\"id\",axis=1)\n",
    "(df.head(699))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f60fc",
   "metadata": {},
   "source": [
    "#### Split the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7067dc",
   "metadata": {},
   "source": [
    "(d) In here, you will split the data into training and test sets, **without using sklearn library.** Please use 20% of the dataset as test set and the rest for train set. Shuffle the data prior to splitting in order to prevent any bias during the training and to avoid the model from learning the order of the training. **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "05e09a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer-wisconsin.data', header=None, names=columns)\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "test_size = int(0.2 * len(df))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = df.iloc[test_size:]  # 80% for training\n",
    "test_data = df.iloc[:test_size]   # 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eda4a2",
   "metadata": {},
   "source": [
    "#### Predict the class for test data and calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173dff6",
   "metadata": {},
   "source": [
    "(e) Now train the KNN classifier you developed in (a) using the training set, and test it on the test set with *k_neighbours=5*. **(4 pts)**\n",
    "\n",
    "Print the confidence for the incorrect predictions the classifier has made.\n",
    "\n",
    "Find the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bad73b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), -1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:158\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), -1)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adde\\Desktop\\MVE137\\MVE137\\Project\\Project.ipynb Cell 41\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming you've already defined the knn_algorithm function and have train_data and test_data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# create KNN model and output the prediction and confidence\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m classification_result, confidence \u001b[39m=\u001b[39m knn_algorithm(train_data, test_data, k_neighbors\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Adde\\Desktop\\MVE137\\MVE137\\Project\\Project.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mknn_algorithm\u001b[39m(train_data, test_data, k_neighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(train_data[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m k_neighbors \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mk is greater than or equal to the number of data points in the training set. Consider choosing a smaller k.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[0;32m   3803\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:5974\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5970\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   5971\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5972\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5973\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5974\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), -1)"
     ]
    }
   ],
   "source": [
    "# Assuming you've already defined the knn_algorithm function and have train_data and test_data\n",
    "\n",
    "# create KNN model and output the prediction and confidence\n",
    "classification_result, confidence = knn_algorithm(train_data, test_data, k_neighbors=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f358925",
   "metadata": {},
   "source": [
    "**Effect of reduction in training data size on confidence of predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5137829",
   "metadata": {},
   "source": [
    "(f) Now increase the test set size to 40% of the dataset while keeping the same *k_neighbours* and print the confidence and accuracy of the predictions similar to the previous question. Explain the results in comparison with (e) **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ad9f89bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adde\\Desktop\\MVE137\\MVE137\\Project\\Project.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m true_label \u001b[39m=\u001b[39m y_test[i]         \u001b[39m# True class label\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Use the KNN classifier to make a prediction\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m predicted_label, confidence \u001b[39m=\u001b[39m knn_algorithm(X_train, test_instance, k_neighbours)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Check if the prediction is incorrect\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m predicted_label \u001b[39m!=\u001b[39m true_label:\n",
      "\u001b[1;32mc:\\Users\\Adde\\Desktop\\MVE137\\MVE137\\Project\\Project.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m confidence \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(test_data), dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, test_instance \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_data):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     distances \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(np\u001b[39m.\u001b[39msum((train_data[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m test_instance[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     sorted_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(distances)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adde/Desktop/MVE137/MVE137/Project/Project.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     k_indices \u001b[39m=\u001b[39m sorted_indices[:k_neighbors]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X_train = train_set.iloc[:, 1:-1].values  # Features for training\n",
    "y_train = train_set.iloc[:, -1].values    # Labels for training\n",
    "X_test = test_set.iloc[:, 1:-1].values    # Features for testing\n",
    "y_test = test_set.iloc[:, -1].values      # True labels for testing\n",
    "\n",
    "k_neighbours = 5\n",
    "\n",
    "# Initialize lists to store incorrect predictions and their corresponding confidences\n",
    "incorrect_predictions = []\n",
    "incorrect_confidences = []\n",
    "\n",
    "# Iterate through the test set\n",
    "for i in range(len(X_test)):\n",
    "    test_instance = X_test[i]      # Features of the test instance\n",
    "    true_label = y_test[i]         # True class label\n",
    "    \n",
    "    # Use the KNN classifier to make a prediction\n",
    "    predicted_label, confidence = knn_algorithm(X_train, test_instance, k_neighbours)\n",
    "    \n",
    "    # Check if the prediction is incorrect\n",
    "    if predicted_label != true_label:\n",
    "        incorrect_predictions.append(i)\n",
    "        incorrect_confidences.append(confidence)\n",
    "\n",
    "# Print the confidence for the incorrect predictions\n",
    "print(\"Confidence for incorrect predictions:\")\n",
    "for i, confidence in zip(incorrect_predictions, incorrect_confidences):\n",
    "    print(f\"Sample {i+1}: Confidence = {confidence:.2f}\")\n",
    "\n",
    "# Calculate and print the accuracy of the predictions\n",
    "accuracy = 1 - (len(incorrect_predictions) / len(X_test))\n",
    "print(\"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a84cc8",
   "metadata": {},
   "source": [
    "#### Alternative classification methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Propose alternative classification approaches for this problem and discuss the advantages and disadvantages with respect to KNNs (you don't need to implement them). **(2 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9fac97",
   "metadata": {},
   "source": [
    "Decision Trees:\n",
    "Pros:\n",
    "Interpretability: Decision trees are easy to understand and explain which is good for making quick judgement calls.\n",
    "Feature Selection: Decision trees naturally select important features, helping identify key variables in the dataset.\n",
    "\n",
    "Cons:\n",
    "Overfitting: Decision trees tend to overfit, leading to poor generalization on unseen data. This requires techniques like pruning to mitigate.\n",
    "Instability: Small variations in the training data can result in significantly different decision trees, making them unstable models.\n",
    "k-Nearest Neighbors (k-NN):\n",
    "Pros:\n",
    "Simplicity: k-NN is simple to implement and understand, making it a quick choice for baseline models.\n",
    "Robustness: k-NN can perform well in the presence of noisy data, outliers, or irrelevant features.\n",
    "Cons:\n",
    "Computational Cost: k-NN can be computationally expensive, especially with large datasets, as it requires calculating distances for each prediction.\n",
    "Sensitivity to k: The choice of the hyperparameter k (number of neighbors) significantly impacts results, and selecting the right k can be challenging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
